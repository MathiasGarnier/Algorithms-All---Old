{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9vCLaHadRy"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn9vCgesadR0"
      },
      "source": [
        "Long-Context GRPO for reinforcement learning — train stably at massive sequence lengths. Fine-tune models with up to 7x more context length efficiently. [Read Blog](https://unsloth.ai/docs/new/grpo-long-context)\n",
        "\n",
        "3× faster training with optimized sequence packing — higher throughput with no quality loss.[Read Blog](https://unsloth.ai/docs/new/3x-faster-training-packing)\n",
        "\n",
        "500k context-length fine-tuning — push long-context models further with memory-efficient training. [Read Blog](https://unsloth.ai/docs/new/500k-context-length-fine-tuning)\n",
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wduOfsInadR0"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOISIR LE MODÈLE !!!\n",
        "model_name = \"unsloth/Qwen2.5-VL-7B-Instruct\" # Compétition fermée\n",
        "#model_name = \"unsloth/Qwen3-VL-8B-Instruct\" # Compétion ouverte, exemple de modèle (on peut monter à plus de 8B)\n",
        "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B)-Vision.ipynb\n",
        "\n",
        "# Je me suis fait manger mes crédits au moment où j'essayais de faire tourner Qwen3 (code ne change pas ensuite a priori)"
      ],
      "metadata": {
        "id": "8STErceaWxtO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0NQuP0FcadR0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "if \"2.5\" and \"7B\" in model_name:\n",
        "    print(\"Modèle 2.5, compétition fermée\")\n",
        "    !pip install transformers==4.56.2\n",
        "elif \"Qwen3\" in model_name:\n",
        "    print(\"Modèle 3, compétion ouverte (exemple de modèle)\")\n",
        "    !pip install transformers==4.57.1\n",
        "else:\n",
        "    print(\"Problème modèle\")\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "53059ca7-d3ae-497e-e8da-79a717aedf05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2000118117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m ] # More models at https://huggingface.co/unsloth\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m model, tokenizer = FastVisionModel.from_pretrained(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use 4bit to reduce memory use. False for 16bit LoRA.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mload_in_8bit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m         model, tokenizer = FastBaseModel.from_pretrained(\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/vision.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, auto_config, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# For debugging - we use a download counter to see if environments are not breaking or if HF is down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mget_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local_files_only\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36mget_statistics\u001b[0;34m(local_files_only)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0mdisabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[0m_get_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m     \u001b[0m_get_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"repeat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m     total_memory = (\n\u001b[1;32m   1235\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_get_statistics\u001b[0;34m(statistics, force_download)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mtime_limited_stats_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_with_time_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m                 \u001b[0mtime_limited_stats_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 raise TimeoutError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/rl_environments.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0meffective_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"process\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                 return _run_in_subprocess(func, seconds, args, kwargs,\n\u001b[0m\u001b[1;32m    776\u001b[0m                                           start_method=start_method, kill_grace=kill_grace)\n\u001b[1;32m    777\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/rl_environments.py\u001b[0m in \u001b[0;36m_run_in_subprocess\u001b[0;34m(func, seconds, args, kwargs, start_method, kill_grace)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407, # Changer ça même si ça change rien\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4c2848",
        "outputId": "ab9d6b17-7719-4871-e6e9-562bbd472580"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de12c363",
        "outputId": "c2fb4a4c-d4f4-4886-c799-0bf13403770f"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/EVAHAN/train_data/'\n",
        "\n",
        "##dataset_files = ['Dataset_A.json', 'Dataset_B.json', 'Dataset_C.json']\n",
        "dataset_files = ['Dataset_A.json', 'Dataset_C.json']\n",
        "#dataset_files = ['Dataset_B.json']\n",
        "combined_dataset = []\n",
        "\n",
        "for filename in dataset_files:\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        combined_dataset.extend(data)\n",
        "\n",
        "print(f\"Successfully loaded and combined {len(dataset_files)} datasets.\")\n",
        "print(f\"Total items in combined dataset: {len(combined_dataset)}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded and combined 2 datasets.\n",
            "Total items in combined dataset: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7bd142c",
        "outputId": "0ad007fc-9560-4a16-f8e4-ff9403734860"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ON SE RESTREINT À PEU DE % DU JEU DE DONNÉES pour tester des trucs\n",
        "subset_size = 0.01\n",
        "subset_data, _ = train_test_split(combined_dataset, test_size=1 - subset_size, random_state=42)\n",
        "\n",
        "print(f\"Using a {subset_size*100}% subset of the data: {len(subset_data)} samples\")\n",
        "\n",
        "train_data, temp_data = train_test_split(subset_data, test_size=0.25, random_state=42)\n",
        "\n",
        "test_data, val_data = train_test_split(temp_data, test_size=0.4, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a 1.0% subset of the data: 100 samples\n",
            "Training set size: 75\n",
            "Testing set size: 15\n",
            "Validation set size: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41d8fb16",
        "outputId": "6492681b-ea5a-4062-e1a1-45de31184900"
      },
      "source": [
        "import PIL.Image\n",
        "print(\"PIL.Image imported.\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIL.Image imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b2221e6e",
        "outputId": "b0ee615f-723c-4fef-9bd4-8f2c4fab62c5"
      },
      "source": [
        "from tqdm import tqdm # héhé\n",
        "import json #\n",
        "import os\n",
        "import cv2 # binarisation image?\n",
        "import numpy as np\n",
        "\n",
        "instruction = \"\"\"Analyze the provided image of ancient Chinese texts.\n",
        "\n",
        "**Task Guidelines:**\n",
        "1. **Transcription:** Transcribe the characters into standard Traditional Chinese (Unicode). Do not modernize the grammar.\n",
        "2. **Legibility:** - If a character is an archaic variant, use the standard Traditional Chinese equivalent.\n",
        "    - If a character is completely illegible due to damage, use '□'.\n",
        "3. **Output:** Return ONLY the JSON object below that can be loaded using the loads function.\n",
        "\n",
        "\n",
        "{\n",
        "  \"transcription\": \"TEXT_HERE\",\n",
        "  \"notes\": \"Brief notes on damage/layout\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 4. **Reading Order:** If there is text, the text is generally written vertically (top to bottom) and arranged in columns from right to left. Respect this strict reading order.\n",
        "\n",
        "base_path = '/content/drive/MyDrive/EVAHAN/train_data/'\n",
        "\n",
        "# Original convert_to_conversation function (commented out)\n",
        "# def convert_to_conversation(sample):\n",
        "#     conversation = [\n",
        "#         { \"role\": \"user\",\n",
        "#           \"content\" : [\n",
        "#             {\"type\" : \"text\",  \"text\"  : instruction},\n",
        "#             {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
        "#         },\n",
        "#         { \"role\" : \"assistant\",\n",
        "#           \"content\" : [\n",
        "#             {\"type\" : \"text\",  \"text\"  : sample[\"text\"]} ]\n",
        "#         },\n",
        "#     ]\n",
        "#     return { \"messages\" : conversation }\n",
        "# pass\n",
        "\n",
        "# Adapted convert_to_conversation function\n",
        "def convert_to_conversation_new(sample):\n",
        "    if \"text\" not in sample:\n",
        "        # Skip samples that do not have a 'text' key\n",
        "        return None\n",
        "\n",
        "    image_path = os.path.join(base_path, sample[\"image_path\"])\n",
        "    # PEUT-ÊTRE MODIFIER ÇA ?\n",
        "    try:\n",
        "    #    # Original:\n",
        "      image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "    #    # NEW: Load image using OpenCV\n",
        "    #    image_cv2 = cv2.imread(image_path)\n",
        "    #    if image_cv2 is None:\n",
        "    #        print(f\"Error: Could not load image {image_path}\")\n",
        "    #        return None\n",
        "\n",
        "    #    # Convert to grayscale\n",
        "    #    gray_image = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply adaptive thresholding (binarization) to remove shadows\n",
        "        # ADAPTIVE_THRESH_GAUSSIAN_C: uses a gaussian weighted sum of neighborhood values\n",
        "        # THRESH_BINARY: the type of thresholding applied\n",
        "        # 255: max value to use with THRESH_BINARY\n",
        "        # 11: block size (size of neighborhood to calculate threshold for)\n",
        "        # 2: constant subtracted from the mean or weighted mean\n",
        "    #    binarized_image = cv2.adaptiveThreshold(gray_image, 255,\n",
        "    #                                             cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "    #                                             cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "        # Convert OpenCV image (numpy array) back to PIL Image\n",
        "     #   image = PIL.Image.fromarray(binarized_image).convert(\"RGB\") # Ensure RGB for model input\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None # Skip samples with problematic images\n",
        "\n",
        "    # Construct the assistant's response as a JSON string based on the instruction format\n",
        "    assistant_response_dict = {\n",
        "        \"transcription\": sample[\"text\"],\n",
        "        \"notes\": \"\" # Assuming no 'notes' provided in the raw dataset, default to empty string\n",
        "    }\n",
        "    assistant_response_json_string = json.dumps(assistant_response_dict, ensure_ascii=False) # ensure_ascii=False to preserve Chinese characters\n",
        "\n",
        "    conversation = [\n",
        "        { \"role\": \"user\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
        "            {\"type\" : \"image\", \"image\" : image} ]\n",
        "        },\n",
        "        { \"role\" : \"assistant\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : assistant_response_json_string} ]\n",
        "        },\n",
        "    ]\n",
        "    return { \"messages\" : conversation }\n",
        "\n",
        "# Original application of convert_to_conversation (commented out)\n",
        "# converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
        "\n",
        "# Apply the new function to the training and validation datasets\n",
        "# Filter out None values in case of image loading errors or missing 'text' key\n",
        "converted_train_dataset = [convert_to_conversation_new(sample) for sample in tqdm(train_data) if convert_to_conversation_new(sample) is not None]\n",
        "converted_val_dataset = [convert_to_conversation_new(sample) for sample in tqdm(val_data) if convert_to_conversation_new(sample) is not None]\n",
        "\n",
        "print(f\"Converted training dataset size: {len(converted_train_dataset)}\")\n",
        "print(f\"Converted validation dataset size: {len(converted_val_dataset)}\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:02<00:00, 34.24it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 23.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted training dataset size: 75\n",
            "Converted validation dataset size: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_HI1nq4g8AT",
        "outputId": "b566663c-e9eb-4202-e459-6e0115bb8d87"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image_path': 'Dataset_A/a_2063.jpg', 'text': '慶喜當知以佛十力無二為方便无'}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEjlr-BChdAj",
        "outputId": "79986201-1bfa-4213-bcae-4d391f2054a6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'user',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': 'Analyze the provided image of ancient Chinese texts.\\n\\n**Task Guidelines:**\\n1. **Transcription:** Transcribe the characters into standard Traditional Chinese (Unicode). Do not modernize the grammar.\\n2. **Legibility:** - If a character is an archaic variant, use the standard Traditional Chinese equivalent.\\n    - If a character is completely illegible due to damage, use \\'□\\'.\\n3. **Output:** Return ONLY the JSON object below that can be loaded using the loads function.\\n\\n\\n{\\n  \"transcription\": \"TEXT_HERE\",\\n  \"notes\": \"Brief notes on damage/layout\"\\n}\\n'},\n",
              "    {'type': 'image',\n",
              "     'image': <PIL.Image.Image image mode=RGB size=1911x194>}]},\n",
              "  {'role': 'assistant',\n",
              "   'content': [{'type': 'text',\n",
              "     'text': '{\"transcription\": \"慶喜當知以佛十力無二為方便无\", \"notes\": \"\"}'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06b60066",
        "outputId": "f0573237-7fd9-4c5e-f625-57de13e3f23b"
      },
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = converted_train_dataset, # Updated to use the new training dataset\n",
        "    eval_dataset = converted_val_dataset,    # Added for evaluation\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # Set num_train_epochs to 1 and max_steps to -1 to train for one full epoch.\n",
        "        # Adjust these values based on desired training duration and dataset size.\n",
        "        max_steps = -1,                    # Set to -1 to train for num_train_epochs\n",
        "        num_train_epochs = 1,              # Train for 1 full epoch\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        max_length = 2048,\n",
        "    ),\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Model does not have a default image size - using 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp0jVaZQnwO5",
        "outputId": "2cde06bf-d5f0-4029-e106-4a2a1995edb9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "13.865 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "d30ea6e6",
        "outputId": "b1c48400-754e-4524-85f2-da9ba5ff4461",
        "collapsed": true
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 75 | Num Epochs = 1 | Total steps = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 51,521,536 of 8,343,688,192 (0.62% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.074700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.749500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.572700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.239400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH2ZJlAWnm-0",
        "outputId": "0e8b5781-7115-47c5-9de5-6b640c2f2977"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97.0848 seconds used for training.\n",
            "1.62 minutes used for training.\n",
            "Peak reserved memory = 13.865 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 94.057 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dced686",
        "outputId": "cb0c0606-022c-4627-e5ba-708b5beed9e2"
      },
      "source": [
        "FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "# Comment out original image and instruction\n",
        "# image = dataset[2][\"image\"]\n",
        "# instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "# Select an example from the test_data split\n",
        "import os\n",
        "import PIL.Image\n",
        "\n",
        "test_example = test_data[0] # Get the first example from the test_data split\n",
        "\n",
        "print(test_example['text'])\n",
        "\n",
        "# Load the image from the test example using its path and the base_path\n",
        "image_path = os.path.join(base_path, test_example[\"image_path\"])\n",
        "image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Use the globally defined instruction for the model\n",
        "instruction = instruction\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "習菩薩摩訶薩行慶喜鼻界鼻界性\n",
            "{\"transcription\": \"聞諸塵網難超行處盡時眠卧想\", \"notes\": \"\"}<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXWKx572rBYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31750642",
        "outputId": "3e6ab93d-2a68-4c6c-a19d-0584abed8109"
      },
      "source": [
        "import os\n",
        "\n",
        "# Ensure the current directory is where we want the files\n",
        "!wget -O task_a_c_eva.py https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_a_c_eva.py\n",
        "!wget -O task_b_eva.py https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_b_eva.py\n",
        "\n",
        "print(\"Downloaded task_a_c_eva.py and task_b_eva.py\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-03 21:52:08--  https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_a_c_eva.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14094 (14K) [text/plain]\n",
            "Saving to: ‘task_a_c_eva.py’\n",
            "\n",
            "\rtask_a_c_eva.py       0%[                    ]       0  --.-KB/s               \rtask_a_c_eva.py     100%[===================>]  13.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2026-02-03 21:52:08 (19.5 MB/s) - ‘task_a_c_eva.py’ saved [14094/14094]\n",
            "\n",
            "--2026-02-03 21:52:08--  https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_b_eva.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20275 (20K) [text/plain]\n",
            "Saving to: ‘task_b_eva.py’\n",
            "\n",
            "task_b_eva.py       100%[===================>]  19.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-03 21:52:09 (139 MB/s) - ‘task_b_eva.py’ saved [20275/20275]\n",
            "\n",
            "Downloaded task_a_c_eva.py and task_b_eva.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e854e219",
        "outputId": "9c0dd9bd-a8e9-4405-db0c-95d18c5129ed"
      },
      "source": [
        "FastVisionModel.for_inference(model) # Ensure model is in inference mode\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image\n",
        "import sys\n",
        "\n",
        "# Ensure the current directory is in sys.path for module discovery\n",
        "if \"/content/\" not in sys.path:\n",
        "    sys.path.append(\"/content/\")\n",
        "\n",
        "# Force reload of the modules if they were previously loaded\n",
        "# This helps ensure the latest version of the script is used after download.\n",
        "if 'task_a_c_eva' in sys.modules:\n",
        "    del sys.modules['task_a_c_eva']\n",
        "if 'task_b_eva' in sys.modules:\n",
        "    del sys.modules['task_b_eva']\n",
        "\n",
        "# Import the evaluation functions\n",
        "from task_a_c_eva import calculate_char_metrics # Corrected import\n",
        "from task_b_eva import LayoutEvaluator\n",
        "\n",
        "print(\"Comparing model inference with ground truth and evaluating for the first 50 test examples:\")\n",
        "\n",
        "sum = 0\n",
        "idx = 0\n",
        "\n",
        "# Iterate through the first 20 examples of the test_data\n",
        "for i, test_example in tqdm(enumerate(test_data[:10]), total=10, desc=\"Performing inference and evaluation\"):\n",
        "    # a. Load the image using its image_path\n",
        "    image_path = os.path.join(base_path, test_example[\"image_path\"])\n",
        "    try:\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # b. Construct the messages list for the tokenizer\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    # c. Apply the chat template to messages\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "\n",
        "    # d. Prepare inputs for the model\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens = False,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # e. Generate the model's output\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 128,\n",
        "                       use_cache = True, temperature = 1.5, min_p = 0.1)\n",
        "\n",
        "    # f. Decode the generated tokens to text\n",
        "    generated_text_tokens = outputs[0][len(inputs[\"input_ids\"][0]):]\n",
        "    model_output_raw = tokenizer.decode(generated_text_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # g. Parse the model's text output as a JSON string\n",
        "    model_output_cleaned = model_output_raw.replace(\"<|im_end|>\", \"\").strip()\n",
        "    predicted_transcription = \"N/A (parsing error)\"\n",
        "    try:\n",
        "        # Replace literal newlines with escaped newlines for valid JSON parsing\n",
        "        model_output_for_json = model_output_cleaned.replace('\\n', '\\\\n')\n",
        "        parsed_output = json.loads(model_output_for_json)\n",
        "        predicted_transcription = parsed_output.get(\"transcription\", \"N/A (transcription key missing)\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decoding error for output: {model_output_cleaned} - {e}\")\n",
        "        predicted_transcription = f\"JSON Error: {model_output_cleaned}\"\n",
        "\n",
        "    # h. Extract the ground truth transcription\n",
        "    # Check if 'text' key exists, as Dataset B does not have it for transcription evaluation\n",
        "    ground_truth_transcription = test_example.get(\"text\", \"N/A (ground truth missing)\")\n",
        "\n",
        "    # i. Print the comparison\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Model Output:     {predicted_transcription}\")\n",
        "    print(f\"Ground Truth:     {ground_truth_transcription}\")\n",
        "\n",
        "    # Conditional evaluation based on dataset\n",
        "    dataset_name = image_path.split('/')[-2]\n",
        "    if \"Dataset_B\" in dataset_name:\n",
        "\n",
        "        print(\"Evaluation (Dataset B): Not applicable. Model outputs transcription, but ground truth is for layout detection.\")\n",
        "        # RÉADAPTER POUR DATASET B\n",
        "\n",
        "    else:\n",
        "        # For Dataset A and C, use calculate_char_metrics and extract comprehensive_score\n",
        "        if ground_truth_transcription != \"N/A (ground truth missing)\":\n",
        "            metrics_ac = calculate_char_metrics(ground_truth_transcription, predicted_transcription) # Call the correct function\n",
        "            score_ac = metrics_ac.get(\"comprehensive_score\", \"N/A (score missing)\") # Extract the score\n",
        "            score_cer = metrics_ac.get(\"cer\", \"N/A (score missing)\")\n",
        "            score_precision = metrics_ac.get(\"precision\", \"N/A (score missing)\")\n",
        "            score_recall = metrics_ac.get(\"recall\", \"N/A (score missing)\")\n",
        "            score_f1 = metrics_ac.get(\"f1\", \"N/A (score missing)\")\n",
        "            score_ned = metrics_ac.get(\"ned\", \"N/A (score missing)\")\n",
        "\n",
        "            print(f\"Evaluation (Dataset A/C): Comprehensive Score = {score_ac}\")\n",
        "            print(f\"Precisely: CER={score_cer}, P={score_precision}, R={score_recall}, F1={score_f1}, ned={score_ned}\")\n",
        "            idx += 1\n",
        "            sum += score_ac\n",
        "        else:\n",
        "            print(\"Evaluation (Dataset A/C): Skipping due to missing ground truth.\")\n",
        "\n",
        "\n",
        "print(f\"Mean: {sum / idx}\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing model inference with ground truth and evaluating for the first 50 test examples:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference and evaluation:  10%|█         | 1/10 [00:10<01:30, 10.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "Model Output:     聞善施藥以拯仁愛之情感外騒然\n",
            "Ground Truth:     習菩薩摩訶薩行慶喜鼻界鼻界性\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.0\n",
            "Precisely: CER=1.0, P=0.0, R=0.0, F1=0.0, ned=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  20%|██        | 2/10 [00:18<01:12,  9.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 2 ---\n",
            "Model Output:     種族思想佔代之精神世界以影響此世\n",
            "Ground Truth:     薩摩訶薩行慶喜身界身界性空何\n",
            "Evaluation (Dataset A/C): Comprehensive Score = -0.0032\n",
            "Precisely: CER=1.0714, P=0.0625, R=0.0714, F1=0.0667, ned=0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  30%|███       | 3/10 [00:32<01:19, 11.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 3 ---\n",
            "Model Output:     衛晝皇御兵車至長極於未央宫風道兵渠臣黑侯侯高\n",
            "Ground Truth:     於昌世傾圯之慘亦極於近年盖風俗奢儉相推而時論倚劘難\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.1223\n",
            "Precisely: CER=0.88, P=0.1364, R=0.12, F1=0.1277, ned=0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  40%|████      | 4/10 [00:40<01:00, 10.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 4 ---\n",
            "Model Output:     一切有是人色是染是疾善共敬\n",
            "Ground Truth:     一切世閒天人阿素洛等供養恭敬\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.2889\n",
            "Precisely: CER=0.7143, P=0.3077, R=0.2857, F1=0.2963, ned=0.7143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  50%|█████     | 5/10 [00:47<00:44,  8.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 ---\n",
            "Model Output:     夜闋角\n",
            "Ground Truth:     夜聞角\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.6667\n",
            "Precisely: CER=0.3333, P=0.6667, R=0.6667, F1=0.6667, ned=0.3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  60%|██████    | 6/10 [00:52<00:29,  7.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 6 ---\n",
            "Model Output:     只愁我去恰君歸\n",
            "Ground Truth:     只愁我去恰君歸\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 1.0\n",
            "Precisely: CER=0.0, P=1.0, R=1.0, F1=1.0, ned=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  70%|███████   | 7/10 [00:58<00:21,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 7 ---\n",
            "Model Output:     日重訂要題習聞\n",
            "Ground Truth:     㽞題要訂重叅日\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.0429\n",
            "Precisely: CER=1.0, P=0.1429, R=0.1429, F1=0.1429, ned=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  80%|████████  | 8/10 [01:10<00:17,  8.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 8 ---\n",
            "Model Output:     初夏景物如煙水虛無悠瀄如繚繞飛霧散盡\n",
            "Ground Truth:     石　洞照明𤣥虛靜大師大重陽萬壽宮提點兼宗敎事賜紫焦\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.014\n",
            "Precisely: CER=1.0, P=0.0556, R=0.04, F1=0.0465, ned=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference and evaluation:  90%|█████████ | 9/10 [01:22<00:09,  9.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON decoding error for output: ```json\n",
            "{\n",
            "  \"transcription\": \"日益住日益正益日益足五根五力十\",\n",
            "  \"notes\": \"\"\n",
            "}\n",
            "``` - Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "--- Example 9 ---\n",
            "Model Output:     JSON Error: ```json\n",
            "{\n",
            "  \"transcription\": \"日益住日益正益日益足五根五力十\",\n",
            "  \"notes\": \"\"\n",
            "}\n",
            "```\n",
            "Ground Truth:     四念住四正断四神足五根五力七\n",
            "Evaluation (Dataset A/C): Comprehensive Score = -2.0085\n",
            "Precisely: CER=5.1429, P=0.0886, R=0.5, F1=0.1505, ned=0.9114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference and evaluation: 100%|██████████| 10/10 [01:32<00:00,  9.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 10 ---\n",
            "Model Output:     鼎川鼎川分故夔喜田式故説以培\n",
            "Ground Truth:     無二無二分故慶喜由此故說以苦\n",
            "Evaluation (Dataset A/C): Comprehensive Score = 0.3571\n",
            "Precisely: CER=0.6429, P=0.3571, R=0.3571, F1=0.3571, ned=0.6429\n",
            "Mean: 0.04801999999999995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model_QWEN2.5_EVAHAN\")\n",
        "tokenizer.save_pretrained(\"lora_model_QWEN2.5_EVAHAN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtiQO4x-tjj3",
        "outputId": "f0ab33f3-1d48-4548-8ab9-af1203dcc8fd"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[9]['text'])\n",
        "print(json.loads(model_output_cleaned)['transcription'])\n"
      ],
      "metadata": {
        "id": "HgdAHxxibQcD",
        "outputId": "a6612072-5e9e-4352-a08c-1e02405a0f05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "無二無二分故慶喜由此故說以苦\n",
            "鼎川鼎川分故夔喜田式故説以培\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kg3-5i-9vx1P"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}