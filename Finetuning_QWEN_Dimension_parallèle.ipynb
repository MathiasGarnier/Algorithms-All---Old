{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "H100",
      "authorship_tag": "ABX9TyPRevluRQMkKYlWiqMdXBD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathiasGarnier/Algorithms-All---Old/blob/master/Finetuning_QWEN_Dimension_parall%C3%A8le.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = True\n",
        "\n",
        "# ------ Choix du modèle\n",
        "model_name = \"unsloth/Qwen2.5-VL-7B-Instruct\" # Compétition fermée\n",
        "#model_name = \"unsloth/Qwen3-VL-8B-Instruct\" # Compétion ouverte, exemple de modèle (on peut monter à plus de 8B)"
      ],
      "metadata": {
        "id": "s46eirTKrj18"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zI2PTRs4sBSk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# ------ TÉLÉCHARGEMENT MÉTRIQUES EVAHAN\n",
        "if DEBUG: print(f\"{\"*\"*12} TÉLÉCHARGEMENT MÉTRIQUES {\"*\"*12}\")\n",
        "!wget -O task_a_c_eva.py https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_a_c_eva.py\n",
        "!wget -O task_b_eva.py https://raw.githubusercontent.com/GoThereGit/EvaHan/refs/heads/main/task_b_eva.py\n",
        "\n",
        "print(\"Downloaded task_a_c_eva.py and task_b_eva.py\")\n",
        "\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "if \"Qwen2.5\" and \"7B\" in model_name:\n",
        "    print(\"Modèle 2.5, compétition fermée\")\n",
        "    !pip install transformers==4.56.2\n",
        "elif \"Qwen3\" in model_name:\n",
        "    print(\"Modèle 3, compétion ouverte (exemple de modèle)\")\n",
        "    !pip install transformers==4.57.1\n",
        "else:\n",
        "    print(\"Problème modèle\")\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import PIL.Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from unsloth import FastVisionModel\n",
        "from torch.utils.data import Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from task_a_c_eva import calculate_char_metrics\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "\n",
        "\n",
        "\n",
        "# ------ CHEMIN D'ACCÈS\n",
        "if DEBUG: print(f\"{\"*\"*12} CHEMINS D'ACCÈS {\"*\"*12}\")\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/EVAHAN/train_data/'\n",
        "\n",
        "\n",
        "\n",
        "# ------ BONJOUR LE MODÈLE\n",
        "if DEBUG: print(f\"{\"*\"*12} CHARGEMENT DU MODÈLE {\"*\"*12}\")\n",
        "\n",
        "GPU_WORTH_IT = torch.cuda.is_bf16_supported() # Si mieux qu'un GPU T4: True, sinon False\n",
        "if DEBUG: print(f\"Support BF16 : {GPU_WORTH_IT}\")\n",
        "\n",
        "instruction = \"\"\"Analyze the provided image of ancient Chinese texts.\n",
        "Transcribe the characters into standard Traditional Chinese (Unicode).\n",
        "Do not add any notes, just the transcription.\"\"\"\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit = True,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")\n",
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,                            # Monter à 32 ?\n",
        "    lora_alpha = 16,                   # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,               # 3407 par défaut, changer ça même si ça change rien\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,               # And LoftQ\n",
        "    # target_modules = \"all-linear\",   # Optional now! Can specify a list if needed\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ------ SÉLECTION DU JEU DE DONNÉES ET CHARGEMENT\n",
        "if DEBUG: print(f\"{\"*\"*12} JEU DE DONNÉES {\"*\"*12}\")\n",
        "\n",
        "subset_size = 1 # 0.1 # pour tests, on se restreint à une petite partie du jeu de données\n",
        "                # sinon mettre à 1\n",
        "assert(subset_size >= 0 and subset_size <= 1)\n",
        "\n",
        "VERIFY_DATASET = True # pour éviter problèmes avec images corrompues\n",
        "\n",
        "if DEBUG: print(f\"\\t\\tRestrict to {subset_size*100}% of the dataset\")\n",
        "\n",
        "dataset_files = ['Dataset_A.json', 'Dataset_C.json']\n",
        "combined_dataset = []\n",
        "\n",
        "for filename in dataset_files:\n",
        "    filepath = os.path.join(base_path, filename)\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        combined_dataset.extend(data)\n",
        "\n",
        "print(f\"Successfully loaded and combined {len(dataset_files)} datasets.\")\n",
        "print(f\"Total items in combined dataset: {len(combined_dataset)}\")\n",
        "\n",
        "def check_single_image(item, base_path):\n",
        "    \"\"\"Fonction interne pour vérifier une seule image.\"\"\"\n",
        "    img_path = os.path.join(base_path, item[\"image_path\"])\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with PIL.Image.open(img_path) as img:\n",
        "            img.verify()\n",
        "        return item\n",
        "    except Exception:\n",
        "        # On peut logger l'erreur ici si besoin\n",
        "        return None\n",
        "\n",
        "def verify_dataset_fast(data_list, base_path, max_workers=20):\n",
        "\n",
        "    if DEBUG: print(f\"Vérification accélérée de {len(data_list)} images...\")\n",
        "\n",
        "    valid_data = []\n",
        "\n",
        "    # max_workers=10 : compromis pour ne pas saturer le Drive\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # On lance toutes les tâches\n",
        "        futures = [executor.submit(check_single_image, item, base_path) for item in data_list]\n",
        "\n",
        "        # On récupère les résultats avec une barre de progression\n",
        "        for future in tqdm(futures, total=len(data_list)):\n",
        "            result = future.result()\n",
        "            if result is not None:\n",
        "                valid_data.append(result)\n",
        "\n",
        "    return valid_data\n",
        "\n",
        "if VERIFY_DATASET:\n",
        "  combined_dataset = verify_dataset_fast(combined_dataset, base_path)\n",
        "  print(f\"Images valides restantes : {len(combined_dataset)}\")\n",
        "\n",
        "if subset_size < 1:\n",
        "    # On réduit d'abord la taille totale\n",
        "    combined_dataset, _ = train_test_split(combined_dataset, train_size=subset_size, random_state=42)\n",
        "\n",
        "# On split ensuite en Train / Temp (Val + Test)\n",
        "train_data, temp_data = train_test_split(combined_dataset, test_size=0.20, random_state=42)\n",
        "# On split Temp en Val et Test (50/50 du temp = 10% chacun du total)\n",
        "test_data, val_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "\n",
        "\n",
        "class EvaHanDataset(Dataset):\n",
        "    def __init__(self, data_list, base_path, instruction):\n",
        "        self.data = data_list\n",
        "        self.base_path = base_path\n",
        "        self.instruction = instruction\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        image_path = os.path.join(self.base_path, sample[\"image_path\"])\n",
        "\n",
        "        try:\n",
        "            image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "            assistant_content = sample.get(\"text\", \"\")\n",
        "        except Exception as e:\n",
        "            # Au lieu de récursion, on renvoie un placeholder safe\n",
        "            # Cela permet au batch de continuer sans planter le Trainer\n",
        "            image = PIL.Image.new('RGB', (512, 512), color='black')\n",
        "            assistant_content = \"□\" # caractère neutre pour ne pas fausser l'apprentissage\n",
        "            print(f\"Skipping corrupt image: {image_path}\")\n",
        "\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                { \"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": self.instruction},\n",
        "                    {\"type\": \"image\", \"image\": image}\n",
        "                ]},\n",
        "                { \"role\": \"assistant\", \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": assistant_content}\n",
        "                ]}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "converted_train_dataset = EvaHanDataset(train_data, base_path, instruction)\n",
        "converted_val_dataset = EvaHanDataset(val_data, base_path, instruction)\n",
        "\n",
        "print(f\"Dataset is converted. Train size: {len(converted_train_dataset)}\")\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"Un exemple: {converted_train_dataset[0]['messages'][1]['content'][0]['text']}\")\n",
        "\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = converted_train_dataset, # Updated to use the new training dataset\n",
        "    eval_dataset = converted_val_dataset,    # Added for evaluation\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,   # ou 1 ?\n",
        "        gradient_accumulation_steps = 4,   # ou 8 ?\n",
        "        warmup_ratio = 0.1,                # 10% de warmup au lieu de 5 steps fixes\n",
        "        num_train_epochs = 1,              #\n",
        "        learning_rate = 2e-4,              # Diminuer? 4e-5? 5e-6 ?\n",
        "\n",
        "        bf16 = GPU_WORTH_IT,               # Si GPU T4 ou equiv, on aura False\n",
        "        fp16 = not GPU_WORTH_IT,           # Si GPU T4 ou equiv, on aura True\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 20,                   # Plus fréquent pour voir la courbe\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 20,\n",
        "        load_best_model_at_end = True,\n",
        "\n",
        "        optim = \"adamw_8bit\",              #\n",
        "        weight_decay = 0.01,               # Légère augmentation pour régulariser\n",
        "        lr_scheduler_type = \"cosine\",      # Plus efficace que linear\n",
        "        seed = 3407,                       #\n",
        "        output_dir = \"outputs\",\n",
        "        #report_to = \"none\",\n",
        "\n",
        "        remove_unused_columns = False,     # Obligatoire pour vision\n",
        "        dataset_text_field = \"\",           #\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True}, #\n",
        "        max_length = 2048,                  # monter à 4096 si GPUs disent oui\n",
        "\n",
        "        logging_steps = 1,              # Affiche les stats à CHAQUE étape\n",
        "        logging_strategy = \"steps\",     # Basé sur les pas, pas sur les époques\n",
        "        report_to = \"tensorboard\",      # Ou \"tensorboard\" pour voir des graphiques\n",
        "        disable_tqdm = False,           # Assurez-vous que la barre de progression est active\n",
        "    )\n",
        "    # Unsloth: Model does not have a default image size - using 512\n",
        ")\n",
        "\n",
        "if DEBUG:\n",
        "  gpu_stats = torch.cuda.get_device_properties(0)\n",
        "  start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "  max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "  print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "  print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "\n",
        "\n",
        "# ------ ENTRAÎNER NOTRE CHER MODÈLE\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "  used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "  used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "  used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "  lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "  print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "  print(\n",
        "      f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        "  )\n",
        "  print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "  print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "  print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "  print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        "\n",
        "\n",
        "\n",
        "if DEBUG:\n",
        "  # VISUALISATION LOSS\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Extraire l'historique des logs\n",
        "  history = trainer.state.log_history\n",
        "\n",
        "  train_loss = [x['loss'] for x in history if 'loss' in x]\n",
        "  eval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
        "  steps = [x['step'] for x in history if 'loss' in x]\n",
        "  eval_steps = [x['step'] for x in history if 'eval_loss' in x]\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(steps, train_loss, label='Training Loss', color='blue', alpha=0.6)\n",
        "  if eval_loss:\n",
        "      plt.plot(eval_steps, eval_loss, label='Validation Loss', color='red', marker='o')\n",
        "\n",
        "  #plt.title('Loss - EvaHan Transcription')\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "FastVisionModel.for_inference(model) # Ensure model is in inference mode\n",
        "\n",
        "sum_score = 0\n",
        "med_cer, med_precision, med_recall, med_f1, med_ned = 0, 0, 0, 0, 0\n",
        "idx = 0\n",
        "\n",
        "# On teste juste sur 50 données de test_data\n",
        "for i, test_example in tqdm(enumerate(test_data[:50]), total=50, desc=\"Inférence et évaluation\"):\n",
        "    image_path = os.path.join(base_path, test_example[\"image_path\"])\n",
        "\n",
        "    try:\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur chargement image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Préparation du prompt (l'instruction ne doit plus mentionner de JSON)\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # GÉNÉRATION\n",
        "    # Temperature baissée à 0.1 pour plus de fidélité au texte\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256, # 128 ? 512 ?\n",
        "        use_cache=True,\n",
        "        temperature=0.1,\n",
        "        min_p=0.05\n",
        "    )\n",
        "\n",
        "    generated_text_tokens = outputs[0][len(inputs[\"input_ids\"][0]):]\n",
        "    model_output_raw = tokenizer.decode(generated_text_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # NETTOYAGE SIMPLE (Plus besoin de JSON)\n",
        "    predicted_transcription = model_output_raw.replace(\"<|im_end|>\", \"\").strip()\n",
        "\n",
        "    ground_truth_transcription = test_example.get(\"text\", \"N/A (ground truth missing)\")\n",
        "\n",
        "    print(f\"\\n--- Exemple {i+1} ---\")\n",
        "    print(f\"Sortie Modèle : {predicted_transcription}\")\n",
        "    print(f\"Réalité (GT)  : {ground_truth_transcription}\")\n",
        "\n",
        "    # ÉVALUATION\n",
        "    dataset_name = image_path.split('/')[-2]\n",
        "\n",
        "    if \"Dataset_B\" in dataset_name:\n",
        "        print(\"Évaluation (Dataset B) : Ignoré (Détection de mise en page).\")\n",
        "    else:\n",
        "        if ground_truth_transcription != \"N/A (ground truth missing)\":\n",
        "\n",
        "            # Calcul des métriques sur le texte brut\n",
        "            metrics_ac = calculate_char_metrics(ground_truth_transcription, predicted_transcription)\n",
        "\n",
        "            s_ac = metrics_ac.get(\"comprehensive_score\", 0)\n",
        "            s_cer = metrics_ac.get(\"cer\", 1.0)\n",
        "            s_p = metrics_ac.get(\"precision\", 0)\n",
        "            s_r = metrics_ac.get(\"recall\", 0)\n",
        "            s_f1 = metrics_ac.get(\"f1\", 0)\n",
        "            s_ned = metrics_ac.get(\"ned\", 0)\n",
        "\n",
        "            print(f\"Score : {s_ac} | CER : {s_cer}\")\n",
        "\n",
        "            sum_score += s_ac\n",
        "            med_cer += s_cer\n",
        "            med_precision += s_p\n",
        "            med_recall += s_r\n",
        "            med_f1 += s_f1\n",
        "            med_ned += s_ned\n",
        "            idx += 1\n",
        "        else:\n",
        "            print(\"Évaluation (Dataset A/C) : Ground Truth manquante.\")\n",
        "\n",
        "# Résultats finaux\n",
        "if idx > 0:\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"MOYENNE GÉNÉRALE ({idx} exemples)\")\n",
        "    print(f\"Comprehensive Score : {sum_score / idx:.4f}\")\n",
        "    print(f\"CER (Taux d'erreur) : {med_cer / idx:.4f}\")\n",
        "    print(f\"F1-Score            : {med_f1 / idx:.4f}\")\n",
        "    print(f\"NED (Distance)      : {med_ned / idx:.4f}\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "model.save_pretrained(\"lora_model_QWEN2.5_FINETUNE_EVAHAN__MONACO_PAR_HOUDI\")\n",
        "tokenizer.save_pretrained(\"lora_model_QWEN2.5_FINETUNE_EVAHAN__MONACO_PAR_HOUDI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "Y0JXzGxyrkjZ",
        "outputId": "5a55b681-0359-4a97-c50a-2ef2b08b6505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************ CHEMINS D'ACCÈS ************\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "************ CHARGEMENT DU MODÈLE ************\n",
            "Support BF16 : True\n",
            "==((====))==  Unsloth 2026.1.4: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.32 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "************ JEU DE DONNÉES ************\n",
            "\t\tRestrict to 100% of the dataset\n",
            "Successfully loaded and combined 2 datasets.\n",
            "Total items in combined dataset: 10000\n",
            "Vérification accélérée de 10000 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [04:36<00:00, 36.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images valides restantes : 9999\n",
            "Training set size: 7999\n",
            "Testing set size: 1000\n",
            "Validation set size: 1000\n",
            "Dataset is converted. Train size: 7999\n",
            "Un exemple: 廿世同居豈天㝎浦陽上邑是其家自古相傳山水勝\n",
            "Unsloth: Model does not have a default image size - using 512\n",
            "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.32 GB.\n",
            "13.994 GB of memory reserved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 7,999 | Num Epochs = 1 | Total steps = 1,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 51,521,536 of 8,343,688,192 (0.62% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='185' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 185/1000 18:13 < 1:21:12, 0.17 it/s, Epoch 0.18/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>7.257600</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>7.696500</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>7.183100</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>7.159300</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>7.119700</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>7.004700</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>7.236100</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>7.080400</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>7.156100</td>\n",
              "      <td>7.230670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "Skipping corrupt image: /content/drive/MyDrive/EVAHAN/train_data/Dataset_C/c_0939.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        }
      ]
    }
  ]
}